{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation using Combination of best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from mido import MidiFile, MidiTrack, Message\n",
    "from mido.midifiles.meta import MetaMessage\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mido\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "import random\n",
    "import librosa\n",
    "from music21 import converter, corpus, instrument, midi, note, chord, pitch, stream, tempo\n",
    "import os\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction from model evaluation dataset I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read midis \n",
    "os.getcwd()\n",
    "files_port = []\n",
    "os.chdir(\"/home/girija/Documents/Project_Evaluation/Port/\")\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        files_port.append(mido.MidiFile(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get duration/ note-type\n",
    "duration_port = []\n",
    "note_port = []\n",
    "for mid in files_port:\n",
    "    d = []\n",
    "    n = []\n",
    "    delta = 0\n",
    "    for track in mid.tracks:\n",
    "        for message in track:\n",
    "            if not isinstance(message, MetaMessage):\n",
    "                d.append(message.time)\n",
    "                n.append(message.type)\n",
    "    duration_port.append(d[6:])\n",
    "    note_port.append(n[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pitch value, duration and note type\n",
    "pitch_port = []\n",
    "for mid in files_port:\n",
    "    p = []\n",
    "    for track in mid.tracks:\n",
    "        for message in track:\n",
    "            message = str(message)\n",
    "            lines = message.splitlines()\n",
    "            for line in lines:\n",
    "                if('note_off' in line or 'note_on' in line):\n",
    "                    props = line.split(' ')\n",
    "                    p.append(int(props[3].split('=')[1]))\n",
    "    pitch_port.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get Binary Values for duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "for duration in duration_port:\n",
    "    size = np.sum(np.asarray(duration))\n",
    "    l1.append(size)\n",
    "    \n",
    "minimum_bin = min(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_binary = []\n",
    "for item in duration_port:\n",
    "    tmp = np.zeros(minimum_bin)\n",
    "    i = 0\n",
    "    for duration in item:\n",
    "        if(duration==1 and i<minimum_bin):\n",
    "                tmp[i] = 1\n",
    "        else:\n",
    "            tmp[i+1:duration+1] = 0\n",
    "        i+=duration\n",
    "    port_binary.append(list(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get pitch values (compared with average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find minimum length \n",
    "l = []\n",
    "idx = []\n",
    "for mid in pitch_port:\n",
    "    l.append(len(mid))\n",
    "\n",
    "#This will be the final minimum value to which the tunes will be cropped irrespective of their genre.\n",
    "minimum_pitch = min(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop the notes to minimum length \n",
    "input_data_port = []\n",
    "for mid in pitch_port:\n",
    "    input_data_port.append(mid[0:minimum_pitch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find vector of pitch values considering difference with average\n",
    "diff_notes2p = []\n",
    "for item in input_data_port:\n",
    "    avg = statistics.mean(item)\n",
    "    diff = []\n",
    "    for note in item:\n",
    "        diff.append(note-avg)\n",
    "    diff_notes2p.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add padding so that size of vector is same as duration vector\n",
    "p = minimum_bin - minimum_pitch\n",
    "padding = list(np.zeros(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(diff_notes2p)):\n",
    "    diff_notes2p[i] = diff_notes2p[i]+padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "os.getcwd()\n",
    "files_port = []\n",
    "os.chdir(\"/home/girija/Documents/Project_Evaluation/Port_wav/\")\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        files_port.append(file)\n",
    "        \n",
    "beats_port = []\n",
    "for file in files_port:\n",
    "    y, sr = librosa.load(file)\n",
    "    tempo, beat = librosa.beat.beat_track(y, sr)\n",
    "    beats_port.append(beat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find minimum length\n",
    "l = []\n",
    "idx = []\n",
    "for mid in beats_port:\n",
    "    l.append(len(mid))\n",
    "\n",
    "minimum_beats = min(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop to minimum\n",
    "beats_p = []\n",
    "for item in beats_port:\n",
    "    beats_p.append(list(item[:minimum_beats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding as per duration vector\n",
    "p = minimum_bin - minimum_beats\n",
    "padding = list(np.zeros(p))\n",
    "for i in range(len(beats_p)):\n",
    "    beats_p[i] = beats_p[i]+padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Datapoints:\n",
    "datasets_port = []\n",
    "for i in range(len(port_binary)):\n",
    "    datasets_port.append(np.asarray([port_binary[i],diff_notes2p[i],beats_p[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansClustering(dataset, k, max_itr):\n",
    "    #create an empty list of centroids\n",
    "    centroids = []\n",
    "    #select first 'k' datapoints as the centroids\n",
    "    for i in range(k):\n",
    "        centroids.append(dataset[i])\n",
    "    \n",
    "    for itr in range(max_itr):\n",
    "        clusters = []\n",
    "        #find distance between a data-point and each centroid\n",
    "        for point in dataset:\n",
    "            distances = [np.linalg.norm(point-centroid) for centroid in centroids]\n",
    "            #select centroid at minimum distance\n",
    "            clusters.append(distances.index(min(distances)))\n",
    "\n",
    "        #empty dictionary to store data-points as per cluster numbers    \n",
    "        data = {}\n",
    "        for i in range(k):\n",
    "            data[i] = []\n",
    "        \n",
    "        #append data-points to the list based on cluster number    \n",
    "        for i in range(len(dataset)):\n",
    "            data[clusters[i]].append(dataset[i])\n",
    "        \n",
    "        #update centroids based on mean of data-points in the cluster    \n",
    "        centroids = []\n",
    "        for point in data:\n",
    "            centroids.append(np.mean(data[point],axis = 0))\n",
    "            \n",
    "    return(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation using silhouette analysis\n",
    "def silhouette_analysis(datasets,k, clusters):\n",
    "    a = []\n",
    "    intra_cluster = []\n",
    "    for i in range(k):\n",
    "        tmp = []\n",
    "        #find tunes belonging to a particular cluster\n",
    "        for j in range(len(datasets)):\n",
    "            if(i==clusters[j]):\n",
    "                tmp.append(j)\n",
    "        intra_cluster.append(tmp)\n",
    "    \n",
    "    #find intra-cluster distances of all data-points\n",
    "    for cluster in intra_cluster:\n",
    "        for i in range(len(cluster)):\n",
    "            other_points = cluster[:i]+cluster[i+1:]\n",
    "            dist = []\n",
    "            flag = 0\n",
    "            for point in other_points:\n",
    "                if(len(other_points)>1):\n",
    "                    dist.append(np.linalg.norm(np.asarray(datasets[point])-np.asarray(datasets[cluster[i]])))\n",
    "                else:\n",
    "                    dist.append(np.mean(datasets[point]))\n",
    "            a.append(np.mean(dist))\n",
    "    \n",
    "    #find inter-cluster distances of all data-points\n",
    "    inter_clusters = []\n",
    "    for i in range(len(intra_cluster)):\n",
    "        inter_clusters.append(list(itertools.chain.from_iterable(intra_cluster[:i]+intra_cluster[i+1:])))\n",
    "    \n",
    "    b = []\n",
    "    for i in range(len(intra_cluster)):\n",
    "        for j in range(len(inter_clusters)):\n",
    "            if(i==j):\n",
    "                for given_point in intra_cluster[i]:\n",
    "                    dist = []\n",
    "                    for other_point in inter_clusters[j]:\n",
    "                        dist.append(np.linalg.norm(np.asarray(datasets[given_point])-np.asarray(datasets[other_point])))\n",
    "                    b.append(min(dist))\n",
    "    \n",
    "    #find silhouette co-efficient of each point\n",
    "    result = []\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            if(i==j):\n",
    "                res = (b[i]-a[i])/(max(a[i],b[i]))\n",
    "                if(not math.isnan(res)):\n",
    "                    result.append(res)\n",
    "    return(statistics.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result on Model Evaluation Dataset I using K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(1):\n",
    "    clusters = KMeansClustering(datasets_port, 7, 10)\n",
    "    total+=silhouette_analysis(datasets_port,7, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on Port Data using KMeans: 0.22460686246807102\n"
     ]
    }
   ],
   "source": [
    "print(\"Result on Port Data using KMeans:\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedKMeansClustering(dataset, k, max_itr):\n",
    "    #create an empty list of centroids\n",
    "    centroids = []\n",
    "    #select first 'k' datapoints as the centroids\n",
    "    for i in range(k):\n",
    "        centroids.append(dataset[i])\n",
    "    \n",
    "    for itr in range(max_itr):\n",
    "        clusters = []\n",
    "        \n",
    "        #find distance between a data-point and each centroid\n",
    "        for point in dataset:\n",
    "            distances = []\n",
    "            for centroid in centroids:\n",
    "                #assign weights to distances\n",
    "                dist1 = np.linalg.norm(point[0]-centroid[0])*0.01\n",
    "                dist2 = np.linalg.norm(point[1]-centroid[1])*0.19\n",
    "                dist3 = np.linalg.norm(point[2]-centroid[2])*0.80\n",
    "                distances.append(dist1+dist2+dist3)\n",
    "            #select centroid at minimum distance\n",
    "            clusters.append(distances.index(min(distances)))\n",
    "\n",
    "        #empty dictionary to store data-points as per cluster numbers    \n",
    "        data = {}\n",
    "        for i in range(k):\n",
    "            data[i] = []\n",
    "        \n",
    "        #append data-points to the list based on cluster number    \n",
    "        for i in range(len(dataset)):\n",
    "            data[clusters[i]].append(dataset[i])\n",
    "        \n",
    "        #update centroids based on mean of data-points in the cluster    \n",
    "        centroids = []\n",
    "        for point in data:\n",
    "            centroids.append(np.mean(data[point],axis = 0))\n",
    "            \n",
    "    return(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result on Model Evaluation Dataset I using Weighted K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/girija/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/girija/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(1):\n",
    "    clusters = WeightedKMeansClustering(datasets_port, 7, 200)\n",
    "    total+=silhouette_analysis(datasets_port,7, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on Port Data using Weighted KMeans: 0.21871773421498075\n"
     ]
    }
   ],
   "source": [
    "print(\"Result on Port Data using Weighted KMeans:\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction from Model Evaluation Dataset II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read midis for session\n",
    "os.getcwd()\n",
    "files_session = []\n",
    "os.chdir(\"/home/girija/Documents/Project_Evaluation/session/\")\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        files_session.append(mido.MidiFile(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get duration/ note-type\n",
    "pitch_session = []\n",
    "duration_session = []\n",
    "note_session = []\n",
    "for mid in files_session:\n",
    "    p = []\n",
    "    d = []\n",
    "    n = []\n",
    "    delta = 0\n",
    "    for track in mid.tracks:\n",
    "        for message in track:\n",
    "            if not isinstance(message, MetaMessage):\n",
    "                p.append(message.note)\n",
    "                d.append(message.time)\n",
    "                n.append(message.type)\n",
    "    duration_session.append(d)\n",
    "    note_session.append(n)\n",
    "    pitch_session.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get minimum length\n",
    "l1 = []\n",
    "for duration in duration_session:\n",
    "    size = np.sum(np.asarray(duration))\n",
    "    l1.append(size)\n",
    "    \n",
    "minimum_bin = min(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get duration in binary\n",
    "session_binary = []\n",
    "for item in duration_session:\n",
    "    tmp = np.zeros(minimum_bin)\n",
    "    i = 0\n",
    "    for duration in item:\n",
    "        if(duration==1 and i<minimum_bin):\n",
    "                tmp[i] = 1\n",
    "        else:\n",
    "            tmp[i+1:duration+1] = 0\n",
    "        i+=duration\n",
    "    session_binary.append(list(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find minimum length\n",
    "l = []\n",
    "idx = []\n",
    "for mid in pitch_session:\n",
    "    l.append(len(mid))\n",
    "\n",
    "#This will be the final minimum value to which the tunes will be cropped irrespective of their genre.\n",
    "minimum_pitch = min(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop the notes to minimum length \n",
    "input_data_session = []\n",
    "for mid in pitch_session:\n",
    "    input_data_session.append(mid[0:minimum_pitch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pitch-vector considering difference with average value\n",
    "diff_notes2s = []\n",
    "for item in input_data_session:\n",
    "    avg = statistics.mean(item)\n",
    "    diff = []\n",
    "    for note in item:\n",
    "        diff.append(note-avg)\n",
    "    diff_notes2s.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding as per duration vector\n",
    "p = minimum_bin - minimum_pitch\n",
    "padding = list(np.zeros(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(diff_notes2s)):\n",
    "    diff_notes2s[i] = diff_notes2s[i]+padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all WAV files\n",
    "os.getcwd()\n",
    "files_session = []\n",
    "os.chdir(\"/home/girija/Documents/Project_Evaluation/session_wav/\")\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        files_session.append(file)\n",
    "        \n",
    "beats_session = []\n",
    "for file in files_session:\n",
    "    y, sr = librosa.load(file)\n",
    "    tempo, beat = librosa.beat.beat_track(y, sr)\n",
    "    beats_session.append(beat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find minimum lengths\n",
    "l = []\n",
    "idx = []\n",
    "for mid in beats_session:\n",
    "    l.append(len(mid))\n",
    "\n",
    "minimum_beats = min(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop to minimum\n",
    "beats_s = []\n",
    "for item in beats_session:\n",
    "    beats_s.append(list(item[:minimum_beats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "p = minimum_bin - minimum_beats\n",
    "padding = list(np.zeros(p))\n",
    "for i in range(len(beats_s)):\n",
    "    beats_s[i] = beats_s[i]+padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Datapoints:\n",
    "datasets_session = []\n",
    "for i in range(len(session_binary)):\n",
    "    datasets_session.append(np.asarray([session_binary[i],diff_notes2s[i],beats_s[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result on Model Evaluation Dataset II using K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(1):\n",
    "    clusters = KMeansClustering(datasets_session, 7, 100)\n",
    "    total+=silhouette_analysis(datasets_session,7, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on session data using KMeans: 0.2393260143167816\n"
     ]
    }
   ],
   "source": [
    "print(\"Result on session data using KMeans:\",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedKMeansClustering(dataset, k, max_itr):\n",
    "    #create an empty list of centroids\n",
    "    centroids = []\n",
    "    #select first 'k' datapoints as the centroids\n",
    "    for i in range(k):\n",
    "        centroids.append(dataset[i])\n",
    "    \n",
    "    for itr in range(max_itr):\n",
    "        clusters = []\n",
    "        \n",
    "        #find distance between a data-point and each centroid\n",
    "        for point in dataset:\n",
    "            distances = []\n",
    "            for centroid in centroids:\n",
    "                #assign weights to distances\n",
    "                dist1 = np.linalg.norm(point[0]-centroid[0])*0.33\n",
    "                dist2 = np.linalg.norm(point[1]-centroid[1])*0.33\n",
    "                dist3 = np.linalg.norm(point[2]-centroid[2])*0.34\n",
    "                distances.append(dist1+dist2+dist3)\n",
    "            #select centroid at minimum distance\n",
    "            clusters.append(distances.index(min(distances)))\n",
    "\n",
    "        #empty dictionary to store data-points as per cluster numbers    \n",
    "        data = {}\n",
    "        for i in range(k):\n",
    "            data[i] = []\n",
    "        \n",
    "        #append data-points to the list based on cluster number    \n",
    "        for i in range(len(dataset)):\n",
    "            data[clusters[i]].append(dataset[i])\n",
    "        \n",
    "        #update centroids based on mean of data-points in the cluster    \n",
    "        centroids = []\n",
    "        for point in data:\n",
    "            centroids.append(np.mean(data[point],axis = 0))\n",
    "            \n",
    "    return(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result on Model Evaluation Dataset II using Weighted K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for i in range(1):\n",
    "    clusters = WeightedKMeansClustering(datasets_session, 7, 200)\n",
    "    total+=silhouette_analysis(datasets_session,7, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on Session Data using Weighted KMeans: 0.3164296509359607\n"
     ]
    }
   ],
   "source": [
    "print(\"Result on Session Data using Weighted KMeans:\",total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
